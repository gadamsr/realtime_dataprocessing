ğŸ“ˆ Real-Time Yahoo Finance Data Processing Pipeline
A scalable, real-time data pipeline that ingests, processes, and stores live stock market data using Apache Kafka, Spark, Cassandra, and Airflow, fully containerized with Docker and orchestrated via Docker Compose.

ğŸ”— Repository: github.com/gadamsr/realtime_dataprocessing

ğŸ“Œ Project Overview
This end-to-end real-time data pipeline performs the following:

ğŸ“¥ Fetches live stock data from the Yahoo Finance API

ğŸ”„ Streams the data through Apache Kafka

âš¡ Processes the stream using Spark Structured Streaming

ğŸ—ƒ Stores the processed data in Cassandra

ğŸ”§ Technologies Used
Apache Kafka â€“ Real-time streaming platform

Apache Spark â€“ Structured stream processing

Apache Airflow â€“ Workflow orchestration

Cassandra â€“ Distributed NoSQL database

Docker & Docker Compose â€“ Containerized deployment

PostgreSQL â€“ Metadata database for Airflow

ğŸ“ Project Structure
bash
Copy
Edit
.
â”œâ”€â”€ airflow_dags/              # Custom DAGs for Airflow
â”œâ”€â”€ config/                    # Configuration files
â”œâ”€â”€ docker-compose.yaml        # Docker Compose config
â”œâ”€â”€ logs/                      # Airflow and Spark logs
â”œâ”€â”€ plugins/                   # Airflow plugins
â”œâ”€â”€ scripts/entrypoint.sh      # Shell setup scripts
â”œâ”€â”€ architecture.png           # System architecture diagram
â”œâ”€â”€ dependencies.zip           # Spark job dependencies
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ stream_processor.py        # Spark streaming processor
ğŸš€ Setup Instructions
1. Clone the Repository
bash
Copy
Edit
git clone https://github.com/gadamsr/realtime_dataprocessing.git
cd realtime_dataprocessing
2. Set Environment Variables
bash
Copy
Edit
echo -e "AIRFLOW_UID=$(id -u)" > .env
echo AIRFLOW_UID=50000 >> .env
3. Initialize Airflow
bash
Copy
Edit
docker-compose up airflow-init
4. Start All Services
bash
Copy
Edit
docker-compose up -d
This will launch:

Kafka Broker

Zookeeper

Cassandra

Spark Master & Worker

Airflow Webserver, Scheduler, Triggerer

Kafka UI

PostgreSQL (Airflow metadata DB)

5. Upload Project Files to Spark Container
bash
Copy
Edit
docker cp dependencies.zip spark-master:/dependencies.zip
docker cp stream_processor.py spark-master:/stream_processor.py
6. Access Cassandra
bash
Copy
Edit
docker exec -it cassandra cqlsh -u cassandra -p cassandra localhost 9042
Run the following CQL commands to verify:

sql
Copy
Edit
DESCRIBE KEYSPACES;
SELECT * FROM stock_data_streaming.stock_data;
SELECT COUNT(*) FROM stock_data_streaming.stock_data;
7. Run the Spark Streaming Job
In a new terminal:

bash
Copy
Edit
docker exec -it spark-master spark-submit \
  --packages com.datastax.spark:spark-cassandra-connector_2.12:3.5.1,\
org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \
  --py-files /dependencies.zip \
  /stream_processor.py
ğŸ“Š Interfaces
Airflow UI
ğŸ“ http://localhost:8080
Login:

Username: admin

Password: admin

Kafka UI
ğŸ“ http://localhost:8085

ğŸ“ Notes
Ensure Docker is installed and running.

.env file helps pass the correct user ID to Airflow.

stream_processor.py contains the core Spark streaming logic.

Kafka topics and Airflow DAGs must be defined before running.

Check container status:

bash
Copy
Edit
docker ps -a
Stop all containers:

bash
Copy
Edit
docker-compose down -v
