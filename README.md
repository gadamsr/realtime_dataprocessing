**Real-Time Yahoo Finance Data Processing Pipeline**
A scalable pipeline for streaming stock market data using Kafka, Spark, and Cassandra.
This project demonstrates a real-time data processing pipeline using Apache Airflow, Kafka, Spark, and Cassandra. The pipeline is containerized using Docker and orchestrated via Docker Compose.

ðŸ”—**Repository:** https://github.com/gadamsr/realtime_dataprocessing

ðŸ“Œ**Overview**
This project implements an end-to-end real-time data pipeline that:
1.	Fetches live stock data from Yahoo Finance API
2.	Streams it through Apache Kafka
3.	Processes it using Spark Structured Streaming
4.	Stores analyzed results in Cassandra
   
ðŸ”§ **Technologies Used**

â€¢	**Apache Kafka** â€“ Streaming platform
â€¢	**Apache Spark** â€“ Stream processing
â€¢	**Apache Airflow** â€“ Workflow orchestration
â€¢	**Cassandra** â€“ NoSQL database
â€¢**Docker & Docker Compose** â€“ Containerization
â€¢**PostgreSQL** â€“ Metadata DB for Airflow
 
**Project Structure**
.
â”œâ”€â”€ airflow_dags/ kafka_stream             # Custom DAGs for Airflow
â”œâ”€â”€ config/                    # Configuration files
â”œâ”€â”€ docker-compose.yaml        # Main docker-compose config
â”œâ”€â”€ logs/                      # Airflow and Spark logs
â”œâ”€â”€ plugins/                   # Airflow plugins
â”œâ”€â”€ scripts/entrypoint.sh                   # Shell scripts or setup helpers
â”œâ”€â”€ architecture.png           # System architecture diagram
â”œâ”€â”€ dependencies.zip           # Spark dependencies
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ stream_processor.py        # Spark stream processor

**ðŸš€ Setup Instructions**

**1. Clone the Repository**
git clone https://github.com/gadamsr/realtime_dataprocessing.git
cd realtime_dataprocessing
2. Set Environment Variables
echo -e "AIRFLOW_UID=$(id -u)" > .env
echo AIRFLOW_UID=50000 >> .env

**3. Initialize Airflow**
docker-compose up airflow-init

**4. Start All Services**
docker-compose up -d
This launches the following services:
â€¢	Kafka Broker
â€¢	Zookeeper
â€¢	Cassandra
â€¢	Spark Master & Worker
â€¢	Airflow Webserver, Scheduler, Triggerer
â€¢	Kafka UI
â€¢	PostgreSQL (for Airflow metadata)

**5. Upload Project Files to Spark**
Copy project files into the Spark master container:
docker cp dependencies.zip spark-master:/dependencies.zip
docker cp stream_processor.py spark-master:/stream_processor.py

**6. Access Cassandra**
docker exec -it cassandra cqlsh -u cassandra -p cassandra localhost 9042
check if topic was created 
DESCRIBE KEYSPACES;
Check if data is being saved in Cassandra
SELECT * FROM stock_data_streaming.stock_data;
SELECT COUNT(*) FROM stock_data_streaming.stock_data;

**7. Run the Spark Job Using docker exec**
In a new terminal 
docker exec -it spark-master spark-submit --packages com.datastax.spark:spark-cassandra-connector_2.12:3.5.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 --py-files /dependencies.zip /stream_processor.py

ðŸ“Š **Airflow UI and Kafka UI**
Once all services are up and running, open the Airflow UI at:
http://localhost:8080
http://localhost:8085

Login with:

â€¢**Username:** admin

â€¢**Password:** admin

ðŸ“Œ**Notes**
â€¢	Make sure Docker is properly installed and running.
â€¢	The .env file is used to pass the UID to Docker for Airflow compatibility.
â€¢	stream_processor.py should define your Spark streaming logic.
â€¢	Kafka topics and DAGs must be created appropriately for the data pipeline to function.
â€¢	To check if all containers are up and healthy use docker ps -a 
â€¢	To down the containers use docker-compose down -v 



